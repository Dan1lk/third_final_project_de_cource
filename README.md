# ETL project

Cоздаем автоматизированный пайплайн обработки и анализа данных о продажах, используя стек технологий: 
- PostgreSQL
- ClickHouse 
- Apache Airflow 
- PySpark

Пайплайн выполняет:
- Генерацию реалистичных данных о продажах
- Обработку данных
- Очистку данных
- Аналитику данных
- Загрузку в базы данных

## Инструкция по запуску:
### Собираем Docker-образы
Для этого в терминале пишем
```bash
docker-compose up -d --build
```
## Сервисы
| Сервис     | Адрес                  |
|------------|------------------------|
| Airflow    | http://localhost:8080  |
| PostgreSQL | http://localhost:5432  |
 | ClickHouse | http://localhost:8123  |

## В этом проекте были поставлены задачи:
- Сгенерировать данные о 1 миллионе продаж за последний год.
- Каждая запись о продаже должна содержать следующие поля:
    - sale_id (уникальный идентификатор продажи).
    - customer_id (идентификатор клиента).
    - product_id (идентификатор продукта).
    - quantity (количество купленных товаров).
    - sale_date (дата продажи).
    - sale_amount (сумма продажи, рассчитывается как количество товаров * случайная цена товара).
    - region (регион клиента, один из: North, South, East, West).
- Удалить дубликаты записей о продажах.
- Привести данные к нужным форматам для дальнейшей обработки.
- Создать таблицу для хранения данных о продажах в PostgreSQL.
- Вставить очищенные данные в эту таблицу.
- Выполнить агрегацию данных, используя оконные функции и группировки:
- Подсчитать общее количество продаж и сумму продаж для каждого региона и каждого продукта.
- Рассчитать средний чек (average_sale_amount) по регионам и продуктам.
- Сохранить агрегированные данные в отдельную таблицу в PostgreSQL.
- Перенести агрегированные данные из PostgreSQL в ClickHouse. Добавить дату импорта. 

## По итогу должен получиться следующий pipeline:

1. DAG запускается.

2. Происходит генерация правдоподобных данные о продажах.

3. Выполняется их очистка и предобработка.

4. Загружаются данные в PostgreSQL, выполняется  необходимые аналитические операции.

5. Переносятся агрегированные данные в ClickHouse.

## Для этого использовались:
1. Docker-compose c apache/airflow:2.9.2 с сервисми PostgreSQL, ClickHouse

2. Dockerfile, где добавил к airflow зависимости и требуемые библиотеки, которые прописаны в requirements.txt

3. В файле main.py прописан код выполнения проекта и все это обернуто в DAG.
После запуска docker-compose up, необходимо переместить файл main.py в папку dags